{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea816a14",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409e6f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import (accuracy_score,classification_report,confusion_matrix)\n",
    "from sklearn import metrics\n",
    "\n",
    "import pickle\n",
    "from importlib.resources import path\n",
    "from pathlib import Path\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.utils\n",
    "from pandas import Int64Index, MultiIndex\n",
    "from sklearn import metrics\n",
    "\n",
    "import submission_src.fincrime.solution_centralized as funcs\n",
    "importlib.reload(funcs) \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4048c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80722998",
   "metadata": {},
   "source": [
    "#### Merging swift Traindata with bankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec1dd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\fincrime-federated\n",
      "Total time taken  101.5761559009552  seconds\n"
     ]
    }
   ],
   "source": [
    "dirname = os.path.dirname('D:/fincrime-federated/')\n",
    "os.chdir(dirname)\n",
    "print(os.getcwd())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_dir = 'D:/fincrime-federated/model/fincrime'\n",
    "preds_format_path = 'D:/fincrime-federated/prediction/fincrime/prediction_format'\n",
    "preds_dest_path = 'D:/fincrime-federated/prediction/fincrime/prediction'\n",
    "\n",
    "\n",
    "## train on data\n",
    "datapathjsonString = 'data/fincrime/centralized/train/data.json'\n",
    "swift_data_path = funcs.json_to_dict(datapathjsonString)['swift_data_path']\n",
    "bank_data_path = funcs.json_to_dict(datapathjsonString)['bank_data_path']\n",
    "\n",
    "def json_to_dict(datapathjsonString):\n",
    "    datapathJson = open(datapathjsonString)\n",
    "    datapathDict = json.load(datapathJson)\n",
    "    return datapathDict\n",
    "\n",
    "\n",
    "def load_data(swift_data_path, bank_data_path):\n",
    "    swift_data = pd.read_csv(swift_data_path, index_col=\"MessageId\")\n",
    "    swift_data[\"Timestamp\"] = swift_data[\"Timestamp\"].astype(\"datetime64[ns]\")\n",
    "    bank_data = pd.read_csv(bank_data_path)\n",
    "    return swift_data, bank_data\n",
    "\n",
    "train_data, bank_data = load_data(swift_data_path=swift_data_path, bank_data_path=bank_data_path)\n",
    "\n",
    "# Merging with bank details\n",
    "train_data = pd.merge(train_data,bank_data,left_on=\"OrderingAccount\",right_on=\"Account\",how=\"left\",)\n",
    "train_data = pd.merge(train_data,bank_data,left_on=\"BeneficiaryAccount\",right_on=\"Account\",how=\"left\",\n",
    "                      suffixes=[\"_order\", \"_ben\"],)\n",
    "\n",
    "trainset = train_data\n",
    "print(\"Total time taken \", (time.time() - start_time),\" seconds\")\n",
    "print(trainset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abdb0d1",
   "metadata": {},
   "source": [
    "#### Merging swift Testdata with bankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4cbf92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken  150.54939603805542  seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "datapathjsonString = 'data/fincrime/centralized/test/data.json'\n",
    "swift_data_path = funcs.json_to_dict(datapathjsonString)['swift_data_path']\n",
    "bank_data_path = funcs.json_to_dict(datapathjsonString)['bank_data_path']\n",
    "\n",
    "test_data, bank_data = load_data(swift_data_path=swift_data_path, bank_data_path=bank_data_path)\n",
    "\n",
    "# Merging with bank details\n",
    "test_data = pd.merge(test_data,bank_data,left_on=\"OrderingAccount\",right_on=\"Account\",how=\"left\",)\n",
    "test_data = pd.merge(test_data,bank_data,left_on=\"BeneficiaryAccount\",right_on=\"Account\",how=\"left\",\n",
    "                     suffixes=[\"_order\", \"_ben\"],)\n",
    "\n",
    "testset = test_data\n",
    "print(\"Total time taken \", (time.time() - start_time),\" seconds\")\n",
    "print(testset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5576db",
   "metadata": {},
   "source": [
    "### Dropping Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "777e901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset.dropna(axis=0,how='any',inplace=True)\n",
    "# print(\"trainset shape is:\"trainset.shape)\n",
    "# testset.dropna(axis=0,how='any',inplace=True)\n",
    "# print(\"testset shape is:\"testset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1561a65",
   "metadata": {},
   "source": [
    "### Normalization / Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29948744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stats(train_dataset):\n",
    "#     train_stats = train_dataset.describe()\n",
    "#     train_stats = train_stats.transpose()\n",
    "#     return train_stats\n",
    "\n",
    "# def norm(x):\n",
    "#     train_stats=stats(x)\n",
    "#     return (x - train_stats['mean']) / train_stats['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f66d4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# scaler = RobustScaler(quantile_range=(25, 75))\n",
    "\n",
    "def norm(x):\n",
    "    scaler.fit(x)\n",
    "    return scaler.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35aa747",
   "metadata": {},
   "source": [
    "# Data Prep Functions (Call these functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cbc173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(train_data):\n",
    "    # Feature engineering\n",
    "#     train_data = train_data.dropna(axis=0)\n",
    "    \n",
    "    train_data = create_features(\n",
    "        df=train_data, model_dir=model_dir, map_from_train_set=False\n",
    "    )\n",
    "\n",
    "    # Keep below columns for training and testing\n",
    "    cols_to_keep = [\n",
    "        \"SettlementAmount\",\n",
    "        \"InstructedAmount\",\n",
    "        \"Label\",\n",
    "        \"hour\",\n",
    "        \"Flags_ben\",\n",
    "        \"MissingBenAccount\",\n",
    "        \"MissingOrdAccount\",\n",
    "        \"Sender_hour_frequency\",\n",
    "        # 'sender_currency_amount_average',\n",
    "        \"Sender_Receiver_frequency\",\n",
    "        \"Sender_InstructedCurrency_frequency\",\n",
    "        \"seq\",\n",
    "        # 'receiver_transactions',\n",
    "        \"Receiver_SettlementCurrency_frequency\",\n",
    "        \"Receiver_hour_frequency\",\n",
    "        \"DifferentOrderNum\",\n",
    "        \"DifferentBenNum\",\n",
    "        \"DifferentOrderName\",\n",
    "        \"DifferentBenName\",\n",
    "        \"DifferentOrderStreet\",\n",
    "        \"DifferentBenStreet\",\n",
    "        \"DifferentOrderZip\",\n",
    "        \"DifferentBenZip\",\n",
    "    ]\n",
    "\n",
    "    train_data_2 = train_data[cols_to_keep]\n",
    "    return train_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e26e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, model_dir, map_from_train_set=False):\n",
    "\n",
    "    ## Feature Engineering\n",
    "\n",
    "    # Hour column\n",
    "    df[\"hour\"] = df[\"Timestamp\"].dt.hour\n",
    "\n",
    "    # Hour frequency for each sender\n",
    "    freq_by_features(df=df,level_1=\"Sender\",level_2=\"hour\",model_dir=model_dir,save_as_object=True,\n",
    "                     map_from_train_set=map_from_train_set,)\n",
    "    # Hour frequency for each receiver\n",
    "    freq_by_features(df=df,level_1=\"Receiver\",level_2=\"hour\",model_dir=model_dir,save_as_object=True,\n",
    "                     map_from_train_set=map_from_train_set,)\n",
    "    # Sender-Currency Frequency\n",
    "    freq_by_features(df=df,level_1=\"Sender\",level_2=\"InstructedCurrency\",model_dir=model_dir,save_as_object=True,\n",
    "                     map_from_train_set=map_from_train_set,)\n",
    "    # Receiver-SettledCurrency Frequency\n",
    "    freq_by_features(df=df,level_1=\"Receiver\",level_2=\"SettlementCurrency\",model_dir=model_dir,save_as_object=True,\n",
    "                     map_from_train_set=map_from_train_set,)\n",
    "    # Sender-Receiver Frequency\n",
    "    freq_by_features(df=df,level_1=\"Sender\",level_2=\"Receiver\",model_dir=model_dir,save_as_object=True,\n",
    "                     map_from_train_set=map_from_train_set,)\n",
    "\n",
    "    # # Average Amount per Sender-Currency - not working\n",
    "    # Sender_ICurrency_mean = mean_by_features(df = df, level_1=\"Sender\" , level_2=\"InstructedCurrency\")\n",
    "    # df.loc[:,\"Sender_ICurrency_mean\"] = \\\n",
    "    #     df.loc[:,\"Sender_InstructedCurrency\"].map(Sender_ICurrency_mean)\n",
    "\n",
    "    # Numbering the transactions within a account order - ben - date combination\n",
    "    df = df.sort_values(\n",
    "        by=[\n",
    "            \"SettlementDate\",\n",
    "            \"Sender\",\n",
    "            \"Receiver\",\n",
    "            \"Account_order\",\n",
    "            \"Account_ben\",\n",
    "            \"Timestamp\",\n",
    "        ],\n",
    "        ascending=True,\n",
    "    )\n",
    "    df[\"seq\"] = (\n",
    "        df.groupby(\n",
    "            [\n",
    "                \"SettlementDate\",\n",
    "                \"Sender\",\n",
    "                \"Receiver\",\n",
    "                \"Account_order\",\n",
    "                \"Account_ben\",\n",
    "            ]\n",
    "        ).cumcount()\n",
    "        + 1\n",
    "    )\n",
    "    df[\"seq\"] = df[\"seq\"].replace(np.NAN, 1)\n",
    "\n",
    "    # Flag columns for transactions with missing bank details\n",
    "    df[[\"MissingBenAccount\"]] = 0\n",
    "    df.loc[df[\"Flags_ben\"].isnull(), \"MissingBenAccount\"] = 1\n",
    "    df[[\"MissingOrdAccount\"]] = 0\n",
    "    df.loc[df[\"Flags_order\"].isnull(), \"MissingOrdAccount\"] = 1\n",
    "\n",
    "    # Different sender account number from bank details\n",
    "    df[\"DifferentOrderNum\"] = np.where(\n",
    "        df[\"Account_order\"] == df[\"OrderingAccount\"], 0, 1\n",
    "    )\n",
    "    # Different receiver account number from bank details\n",
    "    df[\"DifferentBenNum\"] = np.where(\n",
    "        df[\"Account_ben\"] == df[\"BeneficiaryAccount\"], 0, 1\n",
    "    )\n",
    "\n",
    "    # Different sender account name from bank details\n",
    "    df[\"DifferentOrderName\"] = np.where(\n",
    "        df[\"Name_order\"] == df[\"OrderingName\"], 0, 1\n",
    "    )\n",
    "    # Different receiver account name from bank details\n",
    "    df[\"DifferentBenName\"] = np.where(\n",
    "        df[\"Name_ben\"] == df[\"BeneficiaryName\"], 0, 1\n",
    "    )\n",
    "\n",
    "    # Different sender account ordering street from bank details\n",
    "    df[\"DifferentOrderStreet\"] = np.where(\n",
    "        df[\"Street_order\"] == df[\"OrderingStreet\"], 0, 1\n",
    "    )\n",
    "    # Different receiver account ordering street from bank details\n",
    "    df[\"DifferentBenStreet\"] = np.where(\n",
    "        df[\"Street_ben\"] == df[\"BeneficiaryStreet\"], 0, 1\n",
    "    )\n",
    "\n",
    "    # Different sender account country code/zip from bank details\n",
    "    df[\"DifferentOrderZip\"] = np.where(\n",
    "        df[\"CountryCityZip_order\"] == df[\"OrderingCountryCityZip\"], 0, 1\n",
    "    )\n",
    "    # Different receiver account country code/zip from bank details\n",
    "    df[\"DifferentBenZip\"] = np.where(\n",
    "        df[\"CountryCityZip_ben\"] == df[\"BeneficiaryCountryCityZip\"], 0, 1\n",
    "    )\n",
    "\n",
    "    # Some missing value treatment\n",
    "    df.loc[df[\"Flags_ben\"].isna(), \"Flags_ben\"] = 99\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac9c78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_by_features(df,level_1,level_2,model_dir,save_as_object=False,map_from_train_set=False,) -> None:\n",
    "    if map_from_train_set == True:\n",
    "        level_1_level_2_frequency = pickle.load(\n",
    "            open(model_dir+\"/\"+ str(level_1 + \"_\" + level_2 + \"_frequency\")+ \".sav\",\"rb\",)\n",
    "        )\n",
    "        _level_1 = df.loc[:, level_1].unique()\n",
    "        _level_2 = df.loc[:, level_2].unique()\n",
    "        df[str(level_1 + \"_\" + level_2)] = df.loc[:, level_1] + df.loc[\n",
    "            :, level_2\n",
    "        ].astype(str)\n",
    "        df.loc[:, str(level_1 + \"_\" + level_2 + \"_frequency\")] = df.loc[\n",
    "            :, str(level_1 + \"_\" + level_2)\n",
    "        ].map(level_1_level_2_frequency)\n",
    "    else:\n",
    "        _level_1 = df.loc[:, level_1].unique()\n",
    "        _level_2 = df.loc[:, level_2].unique()\n",
    "        df[str(level_1 + \"_\" + level_2)] = df.loc[:, level_1] + df.loc[\n",
    "            :, level_2\n",
    "        ].astype(str)\n",
    "        level_1_level_2_frequency = {}\n",
    "        for s in _level_1:\n",
    "            level_1_rows = df[df[level_1] == s]\n",
    "            for h in _level_2:\n",
    "                level_1_level_2_frequency[s + str(h)] = len(\n",
    "                    level_1_rows[level_1_rows.loc[:, level_2] == h]\n",
    "                )\n",
    "\n",
    "        df.loc[:, str(level_1 + \"_\" + level_2 + \"_frequency\")] = df.loc[\n",
    "            :, str(level_1 + \"_\" + level_2)\n",
    "        ].map(level_1_level_2_frequency)\n",
    "\n",
    "        if save_as_object == True:\n",
    "            pickle.dump(\n",
    "                level_1_level_2_frequency,\n",
    "                open(model_dir + \"/\" + str(level_1 + \"_\" + level_2 + \"_frequency\") + \".sav\",\"wb\",),\n",
    "            )\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553d18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "245ad581",
   "metadata": {},
   "source": [
    "## Centralized result using Keras (Executing Main Script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "658b22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test/validation split\n",
    "\n",
    "trainloadset,valloadset=train_test_split(trainset,test_size=0.3)\n",
    "testloader=testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f1e1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3284207, 31)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(trainloadset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd91919c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(705108, 31)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testloader.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e8dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "tl=data_prep(trainloadset)\n",
    "vl=data_prep(valloadset)\n",
    "testl=data_prep(testloader)\n",
    "\n",
    "Y_train=tl.pop('Label')\n",
    "Y_val=vl.pop('Label')\n",
    "Y_test=testl.pop('Label')\n",
    "\n",
    "X_train=norm(tl)\n",
    "X_val=norm(vl)\n",
    "X_test=norm(testl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d2f6dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3415d2386f9d4148a57213c1127c1a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1770fd6b17aa4455b17b0ced5c75b566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/25.7k [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180a22870be8415380ef6e79824c347d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/25.7k [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0921c5111754400b80781fe19cd66151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/25.7k [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711231b1e5db4daabfeb455cd9546edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/25.7k [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b39156073a405abdf44f91595bf460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/25.7k [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Split: \n",
      "22035/22035 - 46s - loss: 693.2631 - accuracy: 0.1202 - precision: 0.0012 - recall: 0.9961 - 46s/epoch - 2ms/step\n",
      "['loss', 'accuracy', 'precision', 'recall']\n",
      "[693.2630615234375, 0.12016740441322327, 0.0012203524820506573, 0.9960578083992004]\n",
      "22035/22035 [==============================] - 40s 2ms/step\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "AUPRC: 0.0012197963341076439\n"
     ]
    }
   ],
   "source": [
    "from tqdm.keras import TqdmCallback\n",
    "total_num = Y_train.shape[0]\n",
    "num1 = np.count_nonzero(Y_train)\n",
    "num0 = total_num - num1\n",
    "\n",
    "# import NN layers and other componenets.\n",
    "tf.random.set_seed(13)\n",
    "\n",
    "# Keras model \n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(32, input_shape = (input_shape,)))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "input_shape = X_train.shape[1]\n",
    "model = build_model(input_shape)\n",
    "learning_rate = 0.001\n",
    "optimizer = 'adam'\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "model.save(\"my_model\")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "logger = TqdmCallback(verbose=2)\n",
    "model.fit(\n",
    "    x=X_train,\n",
    "    y=Y_train,\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    validation_data=(X_val,Y_val),\n",
    "    class_weight={0: num1 / total_num, 1: num0 / total_num},\n",
    "    callbacks=[early_stopping, logger],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "\n",
    "print('Test Split: ')\n",
    "results =  model.evaluate(X_test, Y_test, verbose=2)\n",
    "print(model.metrics_names)\n",
    "print(results)\n",
    "\n",
    "\n",
    "# Prediction and AUPRC score calcualtion\n",
    "y_pred = model.predict(X_test)\n",
    "ynew = np.round(y_pred).astype(int)\n",
    "print(ynew)\n",
    "print(\"AUPRC:\", metrics.average_precision_score(y_true=Y_test, y_score=ynew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84836c11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
